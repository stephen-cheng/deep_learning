{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to train our classifier to distinguish pictures of cats and dogs. The dataset were are going to work with was published in this [challenge](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "* Download the training data from [here](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data) (train.zip).\n",
    "* Extract and place it in the following directory: **data/cats_v_dogs/train**.\n",
    "* Install **opencv** and **matplotlib** using conda or other Python package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, random, cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "The following blocks of code load the images and prepare them for training. You can treat it as a black box.\n",
    "\n",
    "I borrowed some data preparation functions from [1](https://www.kaggle.com/jeffd23/catdognet-keras-convnet-starter) and [2](https://www.kaggle.com/kbhits/tensorflow-starter-kit-fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"./data/cats_v_dogs/train/\"\n",
    "\n",
    "WIDTH = 64\n",
    "HEIGHT = 64\n",
    "CHANNELS = 3\n",
    "PIXEL_DEPTH = 255\n",
    "\n",
    "NUM_TRAIN = 2500\n",
    "NUM_VALID = 200\n",
    "\n",
    "\n",
    "images = os.listdir(TRAIN_DIR)\n",
    "\n",
    "random.shuffle(images)\n",
    "\n",
    "train_images = images[ : NUM_TRAIN]\n",
    "valid_images = images[NUM_TRAIN : NUM_TRAIN + NUM_VALID]\n",
    "\n",
    "\n",
    "def label_img(img):\n",
    "    word_label = img.split('.')[-3]\n",
    "        \n",
    "    if word_label == 'cat':\n",
    "        return [1,0]\n",
    "    elif word_label == 'dog':\n",
    "        return [0,1]\n",
    "    else:\n",
    "        raise ValueError(\"Something is wrong with the image names. Did you specify the path properly?\")\n",
    "\n",
    "def load_training_data(names):\n",
    "    data = np.empty((len(names), WIDTH, HEIGHT, CHANNELS))\n",
    "    labels = np.empty((len(names), 2), dtype=np.int32)\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        \n",
    "        path = os.path.join(TRAIN_DIR, name)\n",
    "\n",
    "        label = label_img(name)\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "                            \n",
    "        data[i] = img\n",
    "        labels[i] = label\n",
    "        \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "train_data, train_labels = load_training_data(train_images)\n",
    "valid_data, valid_labels = load_training_data(valid_images)\n",
    "\n",
    "train_norm = (train_data[:, :, :] - PIXEL_DEPTH / 2) / PIXEL_DEPTH\n",
    "valid_norm = (valid_data[:, :, :] - PIXEL_DEPTH / 2) / PIXEL_DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, train_data, train_labels, valid_data, valid_labels, batch_size=50):\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.valid_data = valid_data\n",
    "        self.valid_labels = valid_labels\n",
    "        \n",
    "        self.i = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.size = self.train_data.shape[0]\n",
    "        \n",
    "    def get_batch(self):\n",
    "        \n",
    "        ind = range(self.i, self.i + self.batch_size)\n",
    "        \n",
    "        data = self.train_data.take(ind, axis=0, mode=\"wrap\")\n",
    "        labels = self.train_labels.take(ind, axis=0, mode=\"wrap\")\n",
    "        \n",
    "        self.i = (self.i + self.batch_size) % self.size\n",
    "        \n",
    "        return data, labels\n",
    "    \n",
    "    def get_valid_data(self):\n",
    "        return self.valid_data, self.valid_labels\n",
    "    \n",
    "dset = Dataset(train_norm, train_labels, valid_norm, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"A few examples:\")\n",
    "\n",
    "for i in range(3):\n",
    "    plt.imshow(train_data[i].astype(np.uint8), interpolation=\"bicubic\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to define and train a simple convolutional neural network. The network should train for about 2 minutes and achieve about 70% accuracy on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, shape=(None, WIDTH, HEIGHT, CHANNELS))\n",
    "one_hot_labels = tf.placeholder(tf.int64, shape=(None, 2))\n",
    "\n",
    "conv1 = tf.layers.conv2d(input_layer, 16, (5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1, (2, 2), 2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, 32, (5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, (2, 2), 2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1, pool2.get_shape()[1].value * pool2.get_shape()[2].value * pool2.get_shape()[3].value])\n",
    "dense = tf.layers.dense(pool2_flat, units=64, activation=tf.nn.relu)\n",
    "\n",
    "logits = tf.layers.dense(dense, units=2)\n",
    "\n",
    "predictions = tf.argmax(input=logits, axis=1)\n",
    "labels_argmax = tf.argmax(input=one_hot_labels, axis=1)\n",
    "\n",
    "correct = tf.equal(predictions, labels_argmax)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits))\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    data, labels = dset.get_batch()\n",
    "                \n",
    "    tmp_loss, _ = sess.run([loss, train_op], feed_dict = {\n",
    "        input_layer: data,\n",
    "        one_hot_labels: labels\n",
    "    })\n",
    "    \n",
    "    return tmp_loss\n",
    "\n",
    "def valid_step():\n",
    "    \n",
    "    valid_data, valid_labels = dset.get_valid_data()\n",
    "                                    \n",
    "    tmp_accuracy = sess.run(accuracy, feed_dict = {\n",
    "        input_layer: valid_data,\n",
    "        one_hot_labels: valid_labels\n",
    "    })\n",
    " \n",
    "    return tmp_accuracy\n",
    "\n",
    "def baseline():\n",
    "    \n",
    "    _, valid_labels = dset.get_valid_data()\n",
    "        \n",
    "    cats = np.sum(valid_labels[:, 0])\n",
    "    dogs = np.sum(valid_labels[:, 1])\n",
    "\n",
    "    if cats >= dogs:\n",
    "        ratio = cats / (cats + dogs)\n",
    "        print(\"always predict cats accuracy: %.2f%%\" % (ratio * 100))\n",
    "    else:\n",
    "        ratio = dogs / (cats + dogs)\n",
    "        print(\"always predict dogs accuracy: %.2f%%\" % (ratio * 100))\n",
    "        \n",
    "def example_predictions(sess, amount=10):\n",
    "    data, _ = dset.get_valid_data()\n",
    "\n",
    "    data = data[:amount]\n",
    "    labels = sess.run(predictions, feed_dict = {\n",
    "        input_layer: data\n",
    "    })\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "batch_size = 32\n",
    "log_frequency = 100\n",
    "\n",
    "dset.batch_size = batch_size\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        if i != 0 and i % log_frequency == 0:\n",
    "            print(\"training loss: %f\" % (total_loss / log_frequency))\n",
    "            total_loss = 0\n",
    "                                                \n",
    "            tmp_accuracy = valid_step()\n",
    "            \n",
    "            print(\"validation accuracy: %.2f%%\" % (tmp_accuracy * 100))\n",
    "        \n",
    "        total_loss += train_step()\n",
    "        \n",
    "    tmp_accuracy = valid_step()\n",
    "    print(\"\\nfinal accuracy: %.2f%%\" % (tmp_accuracy * 100))\n",
    "    \n",
    "    baseline()\n",
    "    sample_predictions = example_predictions(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if sample_predictions[i] == 0:\n",
    "        print(\"prediction: cat\")\n",
    "    else:\n",
    "        print(\"prediction: dog\")\n",
    "        \n",
    "    plt.imshow(train_data[i].astype(np.uint8), interpolation=\"bicubic\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task (10 points)\n",
    "For this graded task, I'd like you to improve the neural network. Since we're using a very limited amount of training data, our convolutional network might be **overfitting**. We can fix this with **dropout**. Dropout is a form of regularization that drops a certain number of activations during training to make the neural network more robust.\n",
    "\n",
    "1. Implement dropout (10 points). Use the following two links as a reference: [dropout documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [MNIST tutorial (with dropout)](https://www.tensorflow.org/get_started/mnist/pros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(tf.float32, shape=(None, WIDTH, HEIGHT, CHANNELS))\n",
    "one_hot_labels = tf.placeholder(tf.int64, shape=(None, 2))\n",
    "\n",
    "conv1 = tf.layers.conv2d(input_layer, 16, (5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1, (2, 2), 2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, 32, (5, 5), padding=\"same\", activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, (2, 2), 2)\n",
    "\n",
    "pool2_flat = tf.reshape(pool2, [-1, pool2.get_shape()[1].value * pool2.get_shape()[2].value * pool2.get_shape()[3].value])\n",
    "dense = tf.layers.dense(pool2_flat, units=64, activation=tf.nn.relu)\n",
    "\n",
    "\"\"\"\n",
    "TODO: dropout\n",
    "Add dropout after the dense layer.\n",
    "You'll need to add a placeholder to control it (drop 0.5 activations during trainign, don't drop any during validation).\n",
    "\"\"\"\n",
    "\n",
    "logits = tf.layers.dense(dense, units=2)\n",
    "\n",
    "predictions = tf.argmax(input=logits, axis=1)\n",
    "labels_argmax = tf.argmax(input=one_hot_labels, axis=1)\n",
    "\n",
    "correct = tf.equal(predictions, labels_argmax)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits))\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step_with_dropout():\n",
    "    data, labels = dset.get_batch()\n",
    "                    \n",
    "    \"\"\"\n",
    "    TODO: dropout\n",
    "    You should activate dropout in the feed dictionary (keep probability = 0.5).\n",
    "    \"\"\"\n",
    "        \n",
    "    tmp_loss, _ = sess.run([loss_log, loss, train_op], feed_dict = {\n",
    "        input_layer: data,\n",
    "        one_hot_labels: labels\n",
    "    })\n",
    "    \n",
    "    return tmp_loss\n",
    "\n",
    "def valid_step_without_dropout():\n",
    "    \n",
    "    valid_data, valid_labels = dset.get_valid_data()\n",
    "                       \n",
    "    \"\"\"\n",
    "    TODO: dropout\n",
    "    You should deactivate dropout feed dictionary (keep probability = 1).\n",
    "    \"\"\"\n",
    "        \n",
    "    tmp_accuracy = sess.run([accuracy_log, accuracy], feed_dict = {\n",
    "        input_layer: valid_data,\n",
    "        one_hot_labels: valid_labels\n",
    "    })\n",
    " \n",
    "    return tmp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "batch_size = 32\n",
    "log_frequency = 100\n",
    "\n",
    "SUMMARY_DIR = \"summary\"\n",
    "\n",
    "dset.batch_size = batch_size\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        if i != 0 and i % log_frequency == 0:\n",
    "            print(\"training loss: %f\" % (total_loss / log_frequency))\n",
    "            total_loss = 0\n",
    "                                                \n",
    "            tmp_accuracy = valid_step_without_dropout()\n",
    "            \n",
    "            print(\"validation accuracy: %.2f%%\" % (tmp_accuracy * 100))\n",
    "        \n",
    "        total_loss += train_step_with_dropout()\n",
    "                \n",
    "    tmp_accuracy = valid_step_without_dropout()\n",
    "    print(\"\\nfinal accuracy: %.2f%%\" % (tmp_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A\n",
    "\n",
    "** Dropout hasn't improved the accuracy of my model. **\n",
    "\n",
    "There could be two causes:\n",
    "1. You don't disable dropout during validation, which causes the network to perform poorly.\n",
    "2. Your implementation is correct but dropout doesn't help. Dropout is usefull when we're training large neural networks. It's possible that it doesn't improve the training of our small convolutional network or even lowers the validation accuracy. Nevertheless, it's valuable that you know how it works and how to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to go next\n",
    "\n",
    "#### Tensorflow\n",
    "* Learn how to visualize training: https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "* Learn how to save and load models: https://www.tensorflow.org/programmers_guide/variables\n",
    "* Advanced computer vision task: https://www.kaggle.com/c/cifar-10\n",
    "\n",
    "#### Deep Reinforcement Learning\n",
    "* A great series of tutorials: https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
